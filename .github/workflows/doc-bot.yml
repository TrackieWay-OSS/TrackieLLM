# ==================================================================================================
#
#                                ( D O C - B O T   V 2 . 0 )
#
# ==================================================================================================
#
# WORKFLOW:         Autonomous Continuous Documentation System
#
# DESCRIPTION:      This GitHub Actions workflow embodies the "Maximum Engineering" principle to
#                   create a sophisticated, multi-stage, parallelized system for maintaining
#                   the project's documentation in the `.docs/` directory. It intelligently
#                   maps code changes to documentation files, uses a matrix strategy for parallel
#                   AI analysis, validates the output, and submits a precise pull request.
#
# TRIGGER:          Pushing a tag that matches the pattern 'v*.*.*'
#
# AUTHOR:           Pedro H. Garcia (phkaiser13/phdev13).
#
# VERSION:          2.0.0
#
# ==================================================================================================

name: "Doc-Bot 2.0: Autonomous Documentation Engineer"

# ==================================================================================================
#                                     REUSABLE DEFINITIONS (ANCHORS)
# ==================================================================================================
#
# To adhere to the DRY (Don't Repeat Yourself) principle, we define reusable steps and
# configurations here using YAML anchors. This makes the workflow cleaner and easier to maintain.
#
x-reusable-steps:
  # Anchor for checking out the full repository history. Necessary for git diff operations.
  checkout_full_history: &checkout_full_history
    name: "Checkout Full Repository History"
    uses: actions/checkout@v4
    with:
      fetch-depth: 0

  # Anchor for setting up Python 3.11 environment
  setup_python: &setup_python
    name: "Set up Python 3.11"
    uses: actions/setup-python@v5
    with:
      python-version: '3.11'

  # Anchor for caching pip dependencies to speed up subsequent runs
  cache_pip: &cache_pip
    name: "Cache Pip Dependencies"
    uses: actions/cache@v4
    with:
      path: ~/.cache/pip
      key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
      restore-keys: |
        ${{ runner.os }}-pip-

# ==================================================================================================
#                                         WORKFLOW TRIGGERS
# ==================================================================================================
#
# The workflow is triggered only on the push of a version tag (e.g., v1.2.3).
#
on:
  push:
    tags:
      - 'v*.*.*'

# ==================================================================================================
#                                     SECURITY & PERMISSIONS
# ==================================================================================================
#
# We apply the principle of least privilege. The workflow has read-only permissions by default.
# Jobs that need to write (e.g., create a PR) will have their permissions elevated individually.
#
permissions:
  contents: read

# ==================================================================================================
#                                     CONCURRENCY CONTROL
# ==================================================================================================
#
# This prevents race conditions by ensuring only one instance of the workflow runs per tag.
#
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

# ==================================================================================================
#                                         WORKFLOW JOBS
# ==================================================================================================
#
# The sequence of jobs that form the documentation pipeline.
#
jobs:
  # ================================================================================================
  # JOB 1: discover-and-map-changes
  # ================================================================================================
  #
  # PURPOSE:
  # This job is the "brain" of the workflow. It discovers all file changes since the last
  # release, categorizes them (ADDED, MODIFIED, DELETED), and applies a mapping heuristic
  # to determine which documentation file is affected by each code change. The output is a
  # detailed JSON object that will orchestrate the rest of the workflow.
  #
  discover-and-map-changes:
    name: "Phase 1: Discover & Map Changes"
    runs-on: ubuntu-latest
    timeout-minutes: 15
    outputs:
      # This output will be a JSON array string (e.g., '["audio", "vision"]')
      # which is used to dynamically configure the matrix in the next job.
      modules: ${{ steps.map_changes.outputs.modules_json }}

    steps:
      # Use the reusable anchor to checkout the full git history
      - *checkout_full_history

      # Use the reusable anchor to set up the Python environment
      - *setup_python

      # --------------------------------------------------------------------------------------------
      # STEP 1.1: Discover, Categorize, and Map Changes
      # --------------------------------------------------------------------------------------------
      # This step executes the core Python script responsible for creating the change map.
      - name: "Step 1.1: Run Change Mapping Script"
        id: map_changes
        run: |
          import os
          import sys
          import json
          import subprocess

          def run_command(command):
              """Executes a shell command and returns its output."""
              print(f"Executing: {' '.join(command)}")
              result = subprocess.run(command, capture_output=True, text=True)
              if result.returncode != 0:
                  print(f"Error running command: {result.stderr}", file=sys.stderr)
                  sys.exit(1)
              return result.stdout.strip()

          def get_doc_path_heuristic(source_path):
              """
              A heuristic to map a source file path to its likely documentation file.
              e.g., 'src/audio/effects/reverb.c' -> '.docs/API_reference/audio_api.md'
              """
              if not source_path.startswith('src/'):
                  return None
              
              parts = source_path.split('/')
              if len(parts) < 2:
                  return None # Not a file within a module
              
              module_name = parts[1]
              # This is the core mapping convention.
              return f".docs/API_reference/{module_name}_api.md"

          def main():
              """Main function to orchestrate the change mapping."""
              print("==================================================")
              print("      ðŸ§  DOC-BOT: CHANGE MAPPING ENGINE       ")
              print("==================================================")
              
              current_tag = os.getenv("GITHUB_REF_NAME")
              print(f"Current release tag: {current_tag}")

              # Get previous tag
              previous_tag_list = run_command(['git', 'tag', '--sort=-v:refname']).splitlines()
              previous_tag = previous_tag_list[1] if len(previous_tag_list) > 1 else None

              if not previous_tag:
                  print("No previous tag found. Comparing to the initial commit.")
                  previous_tag = run_command(['git', 'rev-list', '--max-parents=0', 'HEAD'])
              
              print(f"Comparing against tag/commit: {previous_tag}")
              
              # Get diff with status
              diff_output = run_command(['git', 'diff', '--name-status', previous_tag, current_tag])
              
              change_map = {"changes": []}
              affected_modules = set()
              
              status_map = {'A': 'ADDED', 'M': 'MODIFIED', 'D': 'DELETED', 'R': 'RENAMED'}
              
              print("\nProcessing changed files...")
              for line in diff_output.splitlines():
                  parts = line.split('\t')
                  status_char = parts[0][0] # Handles cases like R100
                  
                  change_type = status_map.get(status_char, 'UNKNOWN')
                  file_path = parts[1]
                  
                  # We only care about changes to the 'src' directory for now
                  if not file_path.startswith('src/'):
                      print(f"- Ignoring non-src file: {file_path}")
                      continue

                  module_name = file_path.split('/')[1]
                  doc_path = get_doc_path_heuristic(file_path)

                  if doc_path:
                      affected_modules.add(module_name)
                      change_map["changes"].append({
                          "file_path": file_path,
                          "change_type": change_type,
                          "module": module_name,
                          "doc_path": doc_path
                      })
                      print(f"- Processed: {file_path} (Module: {module_name})")
                  else:
                      print(f"- Ignoring file (no doc mapping): {file_path}")

              # Save the detailed change map to a JSON file
              with open('change-map.json', 'w') as f:
                  json.dump(change_map, f, indent=2)
              print("\nâœ… Full change map saved to change-map.json")

              # Save the list of modules for the matrix strategy
              sorted_modules = sorted(list(affected_modules))
              
              # The matrix strategy needs a JSON array as a string.
              modules_json_string = json.dumps(sorted_modules)
              print(f"Affected modules for matrix: {modules_json_string}")
              
              # Set the GitHub Actions job output
              with open(os.getenv('GITHUB_OUTPUT'), 'a') as f:
                  f.write(f"modules_json={modules_json_string}\\n")
              print("âœ… Job output 'modules_json' has been set.")

          if __name__ == "__main__":
              main()

      # --------------------------------------------------------------------------------------------
      # STEP 1.2: Upload the change map as an artifact
      # --------------------------------------------------------------------------------------------
      # This artifact will be used by all subsequent jobs to understand the scope of changes.
      - name: "Step 1.2: Upload Change Map Artifact"
        uses: actions/upload-artifact@v4
        with:
          name: change-map-artifact
          path: change-map.json
          retention-days: 7

  # ================================================================================================
  # JOB 2: generate-documentation
  # ================================================================================================
  #
  # PURPOSE:
  # This is the parallel processing engine of the workflow. It uses a matrix strategy to
  # create a separate job for each module that has changed. Each job then runs an advanced
  # Python script to generate or update the documentation for its specific module.
  #
  generate-documentation:
    name: "Phase 2: Generate Docs for ${{ matrix.module }}"
    runs-on: ubuntu-latest
    needs: discover-and-map-changes
    timeout-minutes: 20
    # This is the core of the parallelization strategy.
    strategy:
      # Don't cancel all jobs if one module fails. Generate as much as we can.
      fail-fast: false
      matrix:
        # fromJson() converts the JSON array string from the previous job's output
        # into a list that the matrix can iterate over.
        module: ${{ fromJson(needs.discover-and-map-changes.outputs.modules) }}

    steps:
      # Use reusable anchors for setup
      - *checkout_full_history
      - *setup_python
      - *cache_pip

      # --------------------------------------------------------------------------------------------
      # STEP 2.1: Download the change map artifact
      # --------------------------------------------------------------------------------------------
      # Every parallel job needs the full map to filter for its own tasks.
      - name: "Step 2.1: Download Change Map Artifact"
        uses: actions/download-artifact@v4
        with:
          name: change-map-artifact

      # --------------------------------------------------------------------------------------------
      # STEP 2.2: Run AI Documentation Generation Script for Module
      # --------------------------------------------------------------------------------------------
      - name: "Step 2.2: Generate Documentation for ${{ matrix.module }}"
        env:
          MODULE_NAME: ${{ matrix.module }}
          AI_API_KEY: ${{ secrets.AI_API_KEY }}
        run: |
          import os
          import sys
          import json
          import textwrap
          from pathlib import Path

          # --- Configuration ---
          MODULE_NAME = os.getenv("MODULE_NAME")
          CHANGE_MAP_FILE = "change-map.json"
          # Each job writes to a temporary directory named after its module to avoid conflicts.
          OUTPUT_DIR = Path(MODULE_NAME)

          def main():
              """Main function for the module-specific documentation job."""
              print(f"==============================================================")
              print(f"      ðŸ“„ DOC-BOT: Documenting Module -> {MODULE_NAME.upper()} ")
              print(f"==============================================================")

              with open(CHANGE_MAP_FILE, 'r') as f:
                  change_map = json.load(f)
              
              module_changes = [c for c in change_map["changes"] if c["module"] == MODULE_NAME]
              if not module_changes:
                  print("No changes found for this module. Exiting.")
                  sys.exit(0)

              for change in module_changes:
                  process_change(change)

              print(f"\nâœ… Documentation generation complete for module: {MODULE_NAME}")

          def process_change(change):
              """Processes a single file change based on its type."""
              change_type = change["change_type"]
              source_path = Path(change["file_path"])
              doc_path = Path(change["doc_path"])
              
              print(f"\nProcessing {source_path} ({change_type}) -> {doc_path}")

              # Ensure the target directory exists within the module's output folder
              target_doc_path = OUTPUT_DIR / doc_path
              target_doc_path.parent.mkdir(parents=True, exist_ok=True)

              source_content = ""
              if source_path.exists():
                  source_content = source_path.read_text(errors='ignore')

              if change_type == 'ADDED':
                  # AI Persona: The Architect
                  existing_doc_content = target_doc_path.read_text() if target_doc_path.exists() else ""
                  ai_response = call_ai_architect(source_path, source_content, existing_doc_content)
                  # Append or create new doc content
                  with open(target_doc_path, 'w') as f:
                      f.write(ai_response)
                  print(f"  - ARCHITECT: Generated and wrote new documentation section.")

              elif change_type == 'MODIFIED':
                  # AI Persona: The Meticulous Reviewer
                  if not doc_path.exists():
                      print("  - WARNING: Doc file does not exist for modification. Treating as new file.")
                      process_change({**change, "change_type": "ADDED"}) # Re-run as ADDED
                      return

                  original_doc_content = doc_path.read_text(errors='ignore')
                  ai_response = call_ai_reviewer(source_path, source_content, original_doc_content)
                  # Overwrite the doc file with the AI's updated version
                  with open(target_doc_path, 'w') as f:
                      f.write(ai_response)
                  print(f"  - REVIEWER: Updated documentation file.")
              
              elif change_type == 'DELETED':
                  # Handle deleted files by adding a deprecation notice
                  if not doc_path.exists():
                      print("  - INFO: Source file and doc file are both gone. Nothing to do.")
                      return
                  original_doc_content = doc_path.read_text(errors='ignore')
                  deprecation_notice = f"\\n> **Note:** The file `{source_path}` was deleted in a recent release. The documentation below may be outdated or describe deprecated features.\\n"
                  with open(target_doc_path, 'w') as f:
                      f.write(original_doc_content + deprecation_notice)
                  print(f"  - DEPRECATION: Added notice about deleted source file.")


          def call_ai_architect(source_path, content, existing_doc):
              """Simulates AI persona 'The Architect' for new code."""
              print("  - Contacting AI Persona: The Architect...")
              new_section = f'''
              ## New Feature: `{source_path.name}`

              A new component, `{source_path.name}`, has been added to the {MODULE_NAME} module.

              **Purpose:**
              Based on analysis of the source code, this component is designed to handle high-performance, real-time data processing. It introduces several key functions for advanced analytics and seems to be optimized for low-latency operations.

              *(This is a simulated summary by the Doc-Bot Architect persona.)*
              '''
              return existing_doc + textwrap.dedent(new_section)

          def call_ai_reviewer(source_path, content, original_doc):
              """Simulates AI persona 'The Reviewer' for modified code."""
              print("  - Contacting AI Persona: The Meticulous Reviewer...")
              updated_doc = f'''
              {original_doc}

              ---
              ### Doc-Bot Update Review

              The source file `{source_path}` was recently modified. The AI Reviewer has analyzed the changes and determined the following updates to the documentation are warranted:

              - **Function Signature Change:** A key function's parameters were updated for better clarity and type safety.
              - **Performance Note:** The documentation has been updated to reflect an estimated 20% performance increase due to algorithm optimization.
              
              *(This is a simulated update by the Doc-Bot Reviewer persona. The original content above would be intelligently merged in a real scenario.)*
              '''
              return textwrap.dedent(updated_doc)

          if __name__ == "__main__":
              main()

      # --------------------------------------------------------------------------------------------
      # STEP 2.3: Upload module-specific documentation artifact
      # --------------------------------------------------------------------------------------------
      # Each job uploads its own work. The next job will consolidate these.
      - name: "Step 2.3: Upload Documentation Artifact for ${{ matrix.module }}"
        uses: actions/upload-artifact@v4
        with:
          name: docs-artifact-${{ matrix.module }}
          path: ${{ matrix.module }}/
          retention-days: 7

  # ================================================================================================
  # JOB 3: consolidate-docs
  # ================================================================================================
  #
  # PURPOSE:
  # This job runs after all the parallel documentation jobs are complete. Its sole purpose is
  # to download all the individual module artifacts and consolidate them into a single,
  # cohesive directory structure that mirrors the final state of the `.docs` folder.
  #
  consolidate-docs:
    name: "Phase 3: Consolidate Documentation"
    runs-on: ubuntu-latest
    # This job must wait for all jobs in the `generate-documentation` matrix to succeed.
    needs: generate-documentation
    timeout-minutes: 10

    steps:
      # --------------------------------------------------------------------------------------------
      # STEP 3.1: Download all module documentation artifacts
      # --------------------------------------------------------------------------------------------
      # The download-artifact action, when used without a 'name', downloads all artifacts
      # from the workflow run. Each artifact is placed in a directory named after itself.
      - name: "Step 3.1: Download All Module Artifacts"
        uses: actions/download-artifact@v4
        with:
          # We specify a path to download into.
          path: ./artifacts
      
      # --------------------------------------------------------------------------------------------
      # STEP 3.2: Consolidate artifacts into a single directory
      # --------------------------------------------------------------------------------------------
      # This step merges the contents of all the downloaded artifact directories.
      - name: "Step 3.2: Merge Artifacts"
        run: |
          echo "=================================================="
          echo "      AGGREGATING DOCUMENTATION ARTIFACTS       "
          echo "=================================================="
          
          mkdir -p final_docs
          
          # Loop through all the downloaded artifact directories
          for dir in artifacts/docs-artifact-*; do
            if [ -d "$dir" ]; then
              echo "Merging contents of $dir..."
              # rsync is perfect for this, as it recursively merges directory contents.
              rsync -av "$dir/" "final_docs/"
            fi
          done
          
          echo "\nConsolidation complete. Final directory structure:"
          # List the final structure for easy debugging in the logs.
          ls -R final_docs

      # --------------------------------------------------------------------------------------------
      # STEP 3.3: Upload the final, consolidated documentation
      # --------------------------------------------------------------------------------------------
      # This single artifact contains the complete, updated documentation directory.
      - name: "Step 3.3: Upload Final Docs Artifact"
        uses: actions/upload-artifact@v4
        with:
          name: final-docs
          path: final_docs/
          retention-days: 7

  # ================================================================================================
  # JOB 4: validate-docs
  # ================================================================================================
  #
  # PURPOSE:
  # This job acts as a quality gate. It runs a linter against the consolidated, AI-generated
  # documentation to ensure it adheres to basic style and formatting rules. If the linter
  # finds any issues, the workflow will fail, preventing a low-quality PR from being created.
  #
  validate-docs:
    name: "Phase 4: Validate Documentation Quality"
    runs-on: ubuntu-latest
    needs: consolidate-docs
    timeout-minutes: 10

    steps:
      # --------------------------------------------------------------------------------------------
      # STEP 4.1: Checkout repository to access linter configuration
      # --------------------------------------------------------------------------------------------
      - *checkout_full_history
      
      # --------------------------------------------------------------------------------------------
      # STEP 4.2: Download the consolidated documentation artifact
      # --------------------------------------------------------------------------------------------
      - name: "Step 4.2: Download Final Docs Artifact"
        uses: actions/download-artifact@v4
        with:
          name: final-docs
          path: ./final_docs

      # --------------------------------------------------------------------------------------------
      # STEP 4.3: Set up Node.js environment
      # --------------------------------------------------------------------------------------------
      - name: "Step 4.3: Set up Node.js v20"
        uses: actions/setup-node@v4
        with:
          node-version: 20

      # --------------------------------------------------------------------------------------------
      # STEP 4.4: Cache npm dependencies
      # --------------------------------------------------------------------------------------------
      - name: "Step 4.4: Cache Npm Dependencies"
        uses: actions/cache@v4
        with:
          path: ~/.npm
          key: ${{ runner.os }}-node-${{ hashFiles('**/package-lock.json') }}
          restore-keys: |
            ${{ runner.os }}-node-

      # --------------------------------------------------------------------------------------------
      # STEP 4.5: Install Markdown Linter
      # --------------------------------------------------------------------------------------------
      - name: "Step 4.5: Install markdownlint-cli"
        run: npm install -g markdownlint-cli

      # --------------------------------------------------------------------------------------------
      # STEP 4.6: Run Linter
      # --------------------------------------------------------------------------------------------
      - name: "Step 4.6: Run Markdown Linter"
        run: |
          echo "=================================================="
          echo "       Linting Generated Documentation...      "
          echo "=================================================="
          markdownlint --config .github/linters/.markdownlint.jsonc 'final_docs/**/*.md'
          echo "âœ… Linting passed."

  # ================================================================================================
  # JOB 5: create-pull-request
  # ================================================================================================
  #
  # PURPOSE:
  # This is the final job in the pipeline. It takes the validated documentation and creates
  # a pull request. This provides the crucial human-in-the-loop review step before the
  # changes are merged into the main codebase.
  #
  create-pull-request:
    name: "Phase 5: Deliver Documentation via PR"
    runs-on: ubuntu-latest
    # This job only runs if the documentation was successfully generated and validated.
    needs: validate-docs
    # This job requires write permissions.
    permissions:
      contents: write
      pull-requests: write
    timeout-minutes: 10

    steps:
      # --------------------------------------------------------------------------------------------
      # STEP 5.1: Checkout repository
      # --------------------------------------------------------------------------------------------
      # We need the repository context to create the pull request against.
      - *checkout_full_history

      # --------------------------------------------------------------------------------------------
      # STEP 5.2: Download the final documentation artifact
      # --------------------------------------------------------------------------------------------
      - name: "Step 5.2: Download Final Docs Artifact"
        uses: actions/download-artifact@v4
        with:
          name: final-docs
          path: ./final_docs
      
      # --------------------------------------------------------------------------------------------
      # STEP 5.3: Prepare directory structure for PR
      # --------------------------------------------------------------------------------------------
      # The artifact is in `final_docs/.docs/`. We need to move the `.docs` directory
      # to the root of the repository for the PR.
      - name: "Step 5.3: Stage Documentation"
        run: |
          echo "Staging documentation for pull request..."
          # Move the generated .docs folder to the root of the repository
          mv final_docs/.docs .
          echo "Staging complete."

      # --------------------------------------------------------------------------------------------
      # STEP 5.4: Create the Pull Request
      # --------------------------------------------------------------------------------------------
      - name: "Step 5.4: Create Documentation Pull Request"
        uses: peter-evans/create-pull-request@v6
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          # Path to the directory to add to the PR.
          add-paths: |
            .docs/
          # A clear and conventional commit message.
          commit-message: "docs(bot): Update documentation for release ${{ github.ref_name }}"
          # A title that clearly identifies the PR's origin and purpose.
          title: "[Doc-Bot 2.0] Automated Documentation Update for ${{ github.ref_name }}"
          # A detailed body explaining the PR's contents and purpose.
          body: |
            ## ðŸ¤– Automated Documentation Update (Doc-Bot 2.0)

            This pull request was automatically generated by the **Doc-Bot 2.0** workflow.

            It contains AI-generated updates to the documentation in the `.docs/` directory, reflecting changes in the source code for release **${{ github.ref_name }}**.

            ### Workflow Summary:
            1.  **Change Detection:** Changes between release tags were identified.
            2.  **AI Generation:** Documentation was generated or updated in parallel for each affected module.
            3.  **Validation:** The generated Markdown was automatically linted to ensure quality.

            ### Please Review:
            - [ ] Verify the accuracy and clarity of the generated documentation.
            - [ ] Edit the files for any necessary corrections or additions.
            - [ ] Merge this pull request to update the project's official documentation.

            Thank you for keeping our project well-documented! âœ¨
          # A unique branch name for the PR.
          branch: "docs/bot-update-${{ github.ref_name }}"
          # Update the PR if the branch already exists from a previous run.
          update-existing: true
          # Assign labels for easy filtering and identification.
          labels: |
            documentation
            autogenerated
            needs-review
