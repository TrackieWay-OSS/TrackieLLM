/*
 * Copyright (C) 2025 Pedro Henrique / phdev13
 *
 * File: src/experiments/metrics_comparator.rs
 *
 * This file provides the logic for comparing performance metrics between two
 * or more AI models. Its primary function is to take a set of performance
 * reports generated by the `model_analysis` module and produce a clear,
 * human-readable comparison that highlights the key differences and trade-offs.
 *
 * The core function, `compare_reports`, identifies a baseline model and then
 * calculates the percentage difference for key metrics (accuracy, latency,
 * throughput) for all other models relative to this baseline. This is
 * essential for A/B testing and regression analysis.
 *
 * Dependencies:
 *   - crate::model_analysis::ModelPerformanceReport: The data structure to be compared.
 *   - log: For structured logging.
 *   - thiserror: For ergonomic error handling.
 *
 * SPDX-License-Identifier: AGPL-3.0 license
 */

use crate::model_analysis::ModelPerformanceReport;
use thiserror::Error;

/// Represents the output of a comparison between multiple model reports.
#[derive(Debug, Clone)]
pub struct ComparisonReport {
    /// The name of the model used as the baseline for comparison.
    pub baseline_model_name: String,
    /// A detailed, human-readable summary of the comparison results.
    pub summary: String,
    // Future versions could include structured data for easier machine processing.
    // pub structured_diff: Vec<MetricDiff>,
}

/// Represents errors that can occur during the metrics comparison process.
#[derive(Debug, Error, PartialEq, Eq)]
pub enum ComparatorError {
    /// Occurs when fewer than two reports are provided for comparison.
    #[error("At least two model reports are required for a comparison.")]
    NotEnoughReports,
}

/// Compares a slice of `ModelPerformanceReport`s and generates a summary.
///
/// This function treats the first report in the slice as the baseline and
/// compares all subsequent reports against it.
///
/// # Arguments
///
/// * `reports` - A slice containing two or more `ModelPerformanceReport`s.
///
/// # Returns
///
/// A `ComparisonReport` containing a summary of the findings.
///
/// # Errors
///
/// Returns `ComparatorError::NotEnoughReports` if `reports` contains fewer
/// than two elements.
pub fn compare_reports(
    reports: &[ModelPerformanceReport],
) -> Result<ComparisonReport, ComparatorError> {
    if reports.len() < 2 {
        return Err(ComparatorError::NotEnoughReports);
    }

    // Use the first report as the baseline for comparison.
    let baseline = &reports[0];
    let mut summary_lines = Vec::new();

    summary_lines.push(format!(
        "Performance Comparison Report (Baseline: {})",
        baseline.model_name
    ));
    summary_lines.push("=".repeat(40));

    for (i, report) in reports.iter().enumerate().skip(1) {
        summary_lines.push(format!("\n--- Comparing with: {} ---", report.model_name));

        // Compare Accuracy
        let acc_diff = (report.accuracy - baseline.accuracy) * 100.0;
        summary_lines.push(format!(
            "Accuracy: {:.2}% (Baseline: {:.2}%) -> Diff: {:+.2}%",
            report.accuracy * 100.0,
            baseline.accuracy * 100.0,
            acc_diff
        ));

        // Compare Average Latency
        let avg_lat_diff =
            (report.average_latency_ms - baseline.average_latency_ms) / baseline.average_latency_ms
                * 100.0;
        summary_lines.push(format!(
            "Avg. Latency (ms): {:.2} (Baseline: {:.2}) -> Diff: {:+.2}% {}",
            report.average_latency_ms,
            baseline.average_latency_ms,
            avg_lat_diff,
            if avg_lat_diff < 0.0 { "(Faster)" } else { "(Slower)" }
        ));

        // Compare P99 Latency
        let p99_lat_diff = (report.latency_percentiles.p99 - baseline.latency_percentiles.p99)
            / baseline.latency_percentiles.p99
            * 100.0;
        summary_lines.push(format!(
            "P99 Latency (ms): {:.2} (Baseline: {:.2}) -> Diff: {:+.2}% {}",
            report.latency_percentiles.p99,
            baseline.latency_percentiles.p99,
            p99_lat_diff,
            if p99_lat_diff < 0.0 { "(Faster)" } else { "(Slower)" }
        ));

        // Compare Throughput (IPS)
        let ips_diff = (report.inferences_per_second - baseline.inferences_per_second)
            / baseline.inferences_per_second
            * 100.0;
        summary_lines.push(format!(
            "Throughput (IPS): {:.2} (Baseline: {:.2}) -> Diff: {:+.2}% {}",
            report.inferences_per_second,
            baseline.inferences_per_second,
            ips_diff,
            if ips_diff > 0.0 {
                "(Higher is Better)"
            } else {
                ""
            }
        ));

        // Compare Peak Memory
        let peak_mem_diff =
            (report.memory_usage.peak_memory_mb as f32 - baseline.memory_usage.peak_memory_mb as f32)
                / (baseline.memory_usage.peak_memory_mb as f32)
                * 100.0;
        summary_lines.push(format!(
            "Peak Memory (MB): {} (Baseline: {}) -> Diff: {:+.2}%",
            report.memory_usage.peak_memory_mb,
            baseline.memory_usage.peak_memory_mb,
            peak_mem_diff
        ));
    }

    let summary = summary_lines.join("\n");
    log::info!("Generated comparison report:\n{}", summary);

    Ok(ComparisonReport {
        baseline_model_name: baseline.model_name.clone(),
        summary,
    })
}
