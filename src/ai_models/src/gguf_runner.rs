/*
 * Copyright (C) 2025 TrackieWay-OSS
 *
 * This file is part of TrackieLLM.
 *
 * TrackieLLM is free software: you can redistribute it and/or modify
 * it under the terms of the GNU Affero General Public License as published by
 * the Free Software Foundation, either version 3 of the License, or
 * (at your option) any later version.
 *
 * TrackieLLM is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 * GNU Affero General Public License for more details.
 *
 * You should have received a copy of the GNU Affero General Public License
 * along with TrackieLLM. If not, see <https://www.gnu.org/licenses/>.
 *
 * SPDX-License-Identifier: AGPL-3.0-or-later
 */

use super::{ffi, loader::Model, AiModelsError, LlmConfig};
use futures::stream::{self, Stream};
use serde::{Deserialize, Serialize};
use std::ffi::{CStr, CString};
use std::ptr::{null_mut, NonNull};

// --- Public Structs and Enums for Streaming API ---

/// Represents a parsed tool call from the LLM.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LlmToolCall {
    pub name: String,
    #[serde(default)]
    pub arguments: serde_json::Value,
}

/// Represents the different events that can be yielded by the LLM stream.
#[derive(Debug)]
pub enum LlmStreamEvent {
    /// A piece of text generated by the model.
    Token(String),
    /// A request from the model to execute a tool.
    ToolCall(LlmToolCall),
    /// The stream has finished generating tokens.
    EndOfStream,
    /// An error occurred during streaming.
    Error(AiModelsError),
}

// --- Internal Handle Management ---

/// A handle to the C-level `tk_llm_runner_t`.
#[derive(Clone, Copy)]
struct RunnerHandle(NonNull<ffi::tk_llm_runner_t>);
unsafe impl Send for RunnerHandle {}

use std::sync::Arc;

/// RAII wrapper for the `tk_llm_runner_t` handle. Its Drop implementation
/// is the key to safe resource management.
struct LlmContext {
    handle: RunnerHandle,
    // Keep a strong reference to the model to ensure it outlives the context.
    _model: Arc<Model>,
}

impl LlmContext {
    fn new(model: Arc<Model>, config: &LlmConfig) -> Result<Self, AiModelsError> {
        let c_system_prompt = CString::new(config.system_prompt)?;
        let c_config = ffi::tk_llm_config_t {
            context_size: config.context_size,
            system_prompt: c_system_prompt.as_ptr(),
            random_seed: config.random_seed,
        };

        let mut ptr = null_mut();
        // The C function now takes the generic model handle directly.
        let code = unsafe { ffi::tk_llm_runner_create(&mut ptr, model.handle.as_ptr(), &c_config) };
        if code != ffi::TK_SUCCESS {
            return Err(AiModelsError::GgufLoadFailed(format!(
                "FFI call to tk_llm_runner_create failed with code {}", code
            )));
        }
        let handle = RunnerHandle(NonNull::new(ptr).expect("C API returned null pointer on success"));
        Ok(Self { handle, _model: model })
    }
}

impl Drop for LlmContext {
    fn drop(&mut self) {
        log::debug!("Dropping LlmContext and destroying C runner.");
        unsafe { ffi::tk_llm_runner_destroy(&mut self.handle.0.as_ptr()) };
    }
}

// --- GGUF Runner Implementation ---

/// A safe, high-level runner for GGUF-based Large Language Models.
pub struct GgufRunner {
    context: LlmContext,
    // This flag tracks if the stream has been consumed. An instance of GgufRunner
    // can only be used to generate one response stream in its lifetime.
    stream_consumed: bool,
}

// Special C pointer value indicating a tool call. Must match the C definition.
const TK_TOOL_CALL_TOKEN: *const i8 = 1 as *const i8;

impl GgufRunner {
    pub fn new(model: Arc<Model>, config: &LlmConfig) -> Result<Self, AiModelsError> {
        let context = LlmContext::new(model, config)?;
        Ok(Self { context, stream_consumed: false })
    }

    pub fn prepare_response(&mut self, prompt: &str, use_tool_grammar: bool) -> Result<(), AiModelsError> {
        if self.stream_consumed {
            return Err(AiModelsError::GgufInferenceFailed("Stream has already been consumed.".to_string()));
        }
        let c_prompt = CString::new(prompt)?;
        let code = unsafe {
            ffi::tk_llm_runner_prepare_generation(self.context.handle.0.as_ptr(), c_prompt.as_ptr(), use_tool_grammar)
        };
        if code != ffi::TK_SUCCESS {
            return Err(AiModelsError::GgufInferenceFailed("Failed to prepare generation".to_string()));
        }
        Ok(())
    }

    /// Consumes the runner to produce a stream of `LlmStreamEvent`s.
    /// The runner cannot be used after this method is called.
    pub fn stream_response(mut self) -> impl Stream<Item = LlmStreamEvent> {
        self.stream_consumed = true;

        stream::unfold(self, |runner| async move {
            let handle = runner.context.handle;

            let event_result = tokio::task::spawn_blocking(move || -> Result<LlmStreamEvent, AiModelsError> {
                let c_str = unsafe { ffi::tk_llm_runner_generate_next_token(handle.0.as_ptr()) };

                if c_str.is_null() {
                    return Ok(LlmStreamEvent::EndOfStream);
                }

                if c_str == TK_TOOL_CALL_TOKEN {
                    let runner_ptr = handle.0.as_ptr();
                    let sctx = unsafe { (*runner_ptr).sctx };
                    let tool_json_cstr = unsafe {
                         let content_ptr = ffi::llama_sampling_get_post_grammar_str(sctx);
                         CStr::from_ptr(content_ptr)
                    };
                    let tool_json = tool_json_cstr.to_str()?;
                    let tool_call: LlmToolCall = serde_json::from_str(tool_json)
                        .map_err(|e| AiModelsError::ToolCallParsingFailed(e.to_string()))?;
                    return Ok(LlmStreamEvent::ToolCall(tool_call));
                }

                let token = unsafe { CStr::from_ptr(c_str) }.to_str()?.to_owned();
                Ok(LlmStreamEvent::Token(token))

            }).await.unwrap(); // `spawn_blocking` only panics if the task does, which is a bug.

            match event_result {
                Ok(LlmStreamEvent::EndOfStream) => None, // Ends the stream, runner is dropped.
                Ok(event) => Some((event, runner)), // Continue with the same runner.
                Err(e) => Some((LlmStreamEvent::Error(e), runner)), // Yield error, next poll will drop.
            }
        })
    }

    /// Adds the result of a tool's execution back into the LLM's context.
    pub fn add_tool_response(&mut self, tool_name: &str, output: &serde_json::Value) -> Result<(), AiModelsError> {
        let c_tool_name = CString::new(tool_name)?;
        let output_str = serde_json::to_string(output)?;
        let c_output = CString::new(output_str)?;

        let code = unsafe {
            ffi::tk_llm_runner_add_tool_response(
                self.context.handle.0.as_ptr(),
                c_tool_name.as_ptr(),
                c_output.as_ptr(),
            )
        };

        if code != ffi::TK_SUCCESS {
            return Err(AiModelsError::GgufInferenceFailed(
                "Failed to add tool response to context".to_string(),
            ));
        }
        Ok(())
    }
}

// The Drop for GgufRunner is now implicitly handled by Rust. When an instance
// goes out of scope, its `context` field (LlmContext) is dropped, which in
// turn calls the C `destroy` function. This happens automatically when the
// stream returned by `stream_response` is finished or dropped.